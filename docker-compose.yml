version: "3.8"

services: 
  redis:
    container_name: redis
    image: redis
    ports:
    - "6379:6379"
  ms_gateway:
    container_name: ms_gateway
    build: ms_gateway
    ports:
    - "5000:5000"
    volumes:
    - ./ms_gateway:/app
  ms_ner:
    build: ms_ner
    container_name: ms_ner
    ports:
    - "5001:5000"
    volumes:
    - ./ms_ner:/app
     
  ms_celery_ner:
    container_name: ms_celery_ner
    build: ms_celery_ner
    volumes:
    - ./ms_celery_ner:/app
    environment:
    - CELERY_BROKER_URL=${CELERY_BROKER_URL}
    - CELERY_BACKEND_URL=${CELERY_BACKEND_URL}
    depends_on:
    - redis 
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    volumes: 
    - ./ms_pyspark/files:/files
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    volumes: 
    - ./ms_pyspark/files:/files
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
  ms_pyspark:
    build: ms_pyspark
    container_name: ms_pyspark
    volumes: 
    - ./ms_pyspark:/app
    depends_on:
      - spark-master
      - spark-worker-1
